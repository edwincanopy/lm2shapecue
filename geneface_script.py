# -*- coding: utf-8 -*-
"""Geneface 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12BRnXhese6Ajbv4MUE_VCfzDTP_aVYyp

Check GPU
"""

import torchvision
import os
os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'

import os
import sys
import torch
import pytorch3d

"""Inference sample"""

!find './' -name 'landmark.txt'

!mkdir localScriptLandmarks

# sample inference, about 3 min
#local_script_landmarks = None

!python inference/genefacepp_infer.py \
--a2m_ckpt=checkpoints/audio2motion_vae \
--head_ckpt= \
--torso_ckpt=checkpoints/motion2video_nerf/may_torso \
--drv_aud=data/raw/val_wavs/MacronSpeech.wav \
--out_name=may_demo.mp4 \
--low_memory_usage

!pwd

"""Display output video"""

# borrow code from makeittalk
from IPython.display import HTML
from base64 import b64encode
import os, sys
import glob

mp4_name = './may_demo.mp4'

mp4 = open('{}'.format(mp4_name),'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

print('Display animation: {}'.format(mp4_name), file=sys.stderr)
display(HTML("""
  <video width=256 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url))

"""Inference"""

# parameters that the model takes in:
# --drv_audio -> the audio to perform inference on

# get checkpoints just from a static image

#

from google.colab import files
img1 = files.upload()

from google.colab.patches import cv2
import matplotlib.pyplot as plt
img = cv2.imread('img1.png')
plt.imshow(img)

!pwd

!ls

!ls inference

!python inference/genefacepp_infer.py --image_path img1.png --output_path img1_infer# --mode landmarks

!ls checkpoints

!python inference/genefacepp_infer.py \
--a2m_ckpt=checkpoints/audio2motion_vae \
--head_ckpt= \
--torso_ckpt= \
--drv_aud=data/raw/val_wavs/MacronSpeech.wav \
--out_name=may_demo.mp4 \
--low_memory_usage

#--torso_ckpt=checkpoints/motion2video_nerf/may_torso \
